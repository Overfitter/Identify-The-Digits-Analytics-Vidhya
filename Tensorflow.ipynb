{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# placeholder: A way to feed data into the graphs\n",
    "# feed_dict: A dictionary to pass numeric values to computational graph\n",
    "# import tensorflow---------------------------------------------------\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build computational graph----------------------------------------\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addition = tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-a2f6003f314f>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# initialize variables---------------------------\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition: 5\n"
     ]
    }
   ],
   "source": [
    "# create session and run the graph-----------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(\"Addition: %i\" % sess.run(addition, feed_dict={a: 2, b: 3}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#  import all the required modules\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let’s set a seed value, so that we can control our models randomness\n",
    "# To stop potential randomness\n",
    "seed = 128\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first step is to set directory paths, for safekeeping!\n",
    "root_dir = os.path.abspath('../..')\n",
    "data_dir = os.path.join(root_dir, 'Deep Learning')\n",
    "sub_dir = os.path.join(root_dir, 'images')\n",
    "\n",
    "# check for existence\n",
    "os.path.exists(root_dir)\n",
    "os.path.exists(data_dir)\n",
    "os.path.exists(sub_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  label\n",
       "0    0.png      4\n",
       "1    1.png      9\n",
       "2    2.png      1\n",
       "3    3.png      7\n",
       "4    4.png      3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let us read our datasets. These are in .csv formats, and have a filename along with the appropriate labels\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('Sample_Submission.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfgglfgj1J/tiN/1Lw2E8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27d01184908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us see what our data looks like! We read our image and display it.\n",
    "img_name = rng.choice(train.filename)\n",
    "filepath = os.path.join(data_dir, 'images','train', img_name)\n",
    "\n",
    "img = imread(filepath, flatten=True)\n",
    "\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,   29.,   85.,   85.,   14.,\n",
       "           0.,    0.,    0.,    0.,    0.,   85.,   85.,   85.,   85.,\n",
       "          85.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,   22.,  168.,  250.,  250.,  146.,\n",
       "         128.,  127.,  127.,  127.,  127.,  252.,  250.,  250.,  250.,\n",
       "         250.,  128.,  127.,   63.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,   43.,  250.,  250.,  250.,  250.,\n",
       "         252.,  250.,  250.,  250.,  250.,  252.,  250.,  250.,  250.,\n",
       "         250.,  252.,  250.,  209.,   56.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,   43.,  250.,  250.,  250.,  250.,\n",
       "         252.,  250.,  250.,  250.,  250.,  252.,  250.,  250.,  250.,\n",
       "         250.,  252.,  250.,  250.,  223.,   34.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,  254.,  252.,  252.,  252.,  252.,\n",
       "         254.,  238.,  210.,  210.,   34.,    0.,    0.,    0.,    0.,\n",
       "           0.,  254.,  252.,  252.,  252.,  252.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,  252.,  250.,  250.,  250.,  250.,\n",
       "         167.,  111.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,  252.,  250.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,  252.,  250.,  250.,  250.,  144.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,  252.,  250.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,  252.,  250.,  250.,  250.,   40.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,  112.,  250.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,  252.,  250.,  250.,  250.,  217.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    7.,  111.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,   36.,  224.,  252.,  252.,  252.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,   86.,  252.,  252.,  252.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,   85.,  250.,  250.,  250.,\n",
       "          85.,   56.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,   85.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,   42.,  188.,  250.,  250.,\n",
       "         252.,  208.,   63.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,  128.,  168.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,  127.,  250.,  250.,\n",
       "         252.,  250.,  209.,  168.,  168.,  170.,  168.,  168.,  168.,\n",
       "         168.,  252.,  250.,  250.,  250.,  250.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,   21.,  181.,  250.,\n",
       "         252.,  250.,  250.,  250.,  250.,  252.,  250.,  250.,  250.,\n",
       "         250.,  252.,  250.,  250.,  250.,   73.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "         211.,  224.,  252.,  252.,  252.,  255.,  252.,  252.,  252.,\n",
       "         252.,  255.,  252.,  231.,   70.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,   56.,  166.,  223.,  250.,  167.,  166.,  166.,  166.,\n",
       "         166.,  167.,  166.,   83.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,   84.,  125.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.],\n",
       "       [   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# he above image is represented as numpy array, as seen below\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier data manipulation, let’s store all our images as numpy arrays-----------------\n",
    "temp= []\n",
    "for img_name in train.filename:\n",
    "    image_path = os.path.join(data_dir,'images', 'train', img_name)\n",
    "    img = imread(image_path, flatten=True)\n",
    "    img = img.astype('float32')\n",
    "    temp.append(img)\n",
    "    \n",
    "train_x = np.stack(temp)\n",
    "\n",
    "temp = []\n",
    "for img_name in test.filename:\n",
    "    image_path = os.path.join(data_dir, 'images', 'test', img_name)\n",
    "    img = imread(image_path, flatten=True)\n",
    "    img = img.astype('float32')\n",
    "    temp.append(img)\n",
    "    \n",
    "test_x = np.stack(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to test the proper functioning of our model we create a validation set. Let’s take a split size of 70:30 for train set vs \n",
    "# validation set\n",
    "split_size = int(train_x.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = train.label.values[:split_size], train.label.values[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34300, 28, 28)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we define some helper functions, which we use later on, in our programs\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    \n",
    "    return labels_one_hot\n",
    "\n",
    "def preproc(unclean_batch_x):\n",
    "    \"\"\"Convert values to range 0-1\"\"\"\n",
    "    temp_batch = unclean_batch_x / unclean_batch_x.max()\n",
    "    \n",
    "    return temp_batch\n",
    "\n",
    "def batch_creator(batch_size, dataset_length, dataset_name):\n",
    "    \"\"\"Create batch with random samples and return appropriate format\"\"\"\n",
    "    batch_mask = rng.choice(dataset_length, batch_size)\n",
    "    \n",
    "    batch_x = eval(dataset_name + '_x')[[batch_mask]].reshape(-1, input_num_units)\n",
    "    batch_x = preproc(batch_x)\n",
    "    \n",
    "    if dataset_name == 'train':\n",
    "        batch_y = eval(dataset_name).ix[batch_mask, 'label'].values\n",
    "        batch_y = dense_to_one_hot(batch_y)\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now comes the main part! Let us define our neural network architecture. We define a neural network with 3 layers;  \n",
    "# input, hidden and output. The number of neurons in input and output are fixed, as the input is our 28 x 28 image and \n",
    "# the output is a 10 x 1 vector representing the class. We take 500 neurons in the hidden layer. This number can vary \n",
    "# according to need. We also assign values to remaining variables. \n",
    "### set all variables\n",
    "\n",
    "# number of neurons in each layer--------------------------\n",
    "input_num_units = 28*28\n",
    "hidden_num_units = 500\n",
    "output_num_units = 10\n",
    "\n",
    "# define placeholders--------------------------------------\n",
    "x = tf.placeholder(tf.float32, [None, input_num_units])\n",
    "y = tf.placeholder(tf.float32, [None, output_num_units])\n",
    "\n",
    "# set remaining variables----------------------------------\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "\n",
    "### define weights and biases of the neural network (refer this article if you don't understand the terminologies)\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([input_num_units, hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([hidden_num_units, output_num_units], seed=seed))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([hidden_num_units], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([output_num_units], seed=seed))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now create our neural networks computational graph------------------------\n",
    "hidden_layer = tf.add(tf.matmul(x, weights['hidden']), biases['hidden'])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, we need to define cost of our neural network\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer,labels= y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And set the optimizer, i.e. our backpropogation algorithm. Here we use Adam, which is an efficient variant of Gradient Descent \n",
    "# algorithm. There are a number of other optimizers available in tensorflow \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-51-c2faa43e3d90>:2: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# After defining our neural network architecture, let’s initialize all the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 8.89807\n",
      "Epoch: 2 cost = 1.82948\n",
      "Epoch: 3 cost = 0.85988\n",
      "Epoch: 4 cost = 0.55418\n",
      "Epoch: 5 cost = 0.43006\n",
      "\n",
      "Training complete!\n",
      "Validation Accuracy: 0.95068\n"
     ]
    }
   ],
   "source": [
    "# Now let us create a session, and run our neural network in the session. We also validate our models accuracy on validation set \n",
    "# that we created\n",
    "with tf.Session() as sess:\n",
    "    # create initialized variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    ### for each epoch, do:\n",
    "    ###   for each batch, do:\n",
    "    ###     create pre-processed batch\n",
    "    ###     run optimizer by feeding batch\n",
    "    ###     find cost and reiterate to minimize\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(train.shape[0]/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = batch_creator(batch_size, train_x.shape[0], 'train')\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_x, y: batch_y})\n",
    "            \n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print(\"Epoch:\", (epoch+1), \"cost =\", \"{:.5f}\".format(avg_cost))\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    \n",
    "    \n",
    "    # find predictions on val set\n",
    "    pred_temp = tf.equal(tf.argmax(output_layer, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(pred_temp, \"float\"))\n",
    "    print(\"Validation Accuracy:\", accuracy.eval({x: val_x.reshape(-1, input_num_units), y: dense_to_one_hot(val_y)}))\n",
    "    \n",
    "    predict = tf.argmax(output_layer, 1)\n",
    "    pred = predict.eval({x: test_x.reshape(-1, input_num_units)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is:  9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27d172b7320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To test our model with our own eyes, let’s visualize its predictions\n",
    "img_name = rng.choice(test.filename)\n",
    "filepath = os.path.join(data_dir, 'images', 'test', img_name)\n",
    "\n",
    "img = imread(filepath, flatten=True)\n",
    "\n",
    "test_index = int(img_name.split('.')[0]) - 49000\n",
    "\n",
    "print(\"Prediction is: \", pred[test_index])\n",
    "\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let’s create a submission\n",
    "sample_submission.filename = test.filename\n",
    "\n",
    "sample_submission.label = pred\n",
    "\n",
    "sample_submission.to_csv(os.path.join(data_dir, 'sub01.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
